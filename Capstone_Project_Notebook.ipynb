{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project Notebook -Clustering stations in the Italian region of Lazio\n",
    "### IBM Data Scientist Certified Capstone by IBM/Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [Business Problem](#businessproblem)\n",
    "* [Data](#data)\n",
    "* [Methodology](#methodology)\n",
    "* [Analysis](#analysis)\n",
    "* [Results and Discussion](#results)\n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction<a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is a \"Capstone Project\" of Coursera - \"IBM Certified Data Scientist\" program.\n",
    "The project is focused on the railway stations located in the Italian region of Lazio, a territory populated by 5.9 million people, visited by more than 12 million tourists each year and served by 165 train stations (41 of them inside the city of Rome).\n",
    "Rete Ferroviaria Italiana is the owner of the stations, and classifies them as “bronze”, “silver”, “gold” and “platinum”, depending on the services offered at the venue, to both travelers and non-travelers.\n",
    "Goal of the project is to use data gathered from Foursquare and other sources to classify the stations independently and understand if some stations should deserve a different status, according to the characteristics of the nearby area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem <a name=\"businessproblem\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFI train stations underwent a significant transformation, from travel hubs to meeting centers and places of aggregation, going beyond their original role of serving travelers by offering diverse services to travelers and non-travelers as well, such as catering services, parking places, retail stores, coworking places and more.\n",
    "The service offering of a station depends on its class, that goes from “bronze” to “silver”, “gold” and “platinum”.\n",
    "Goal of this study is to use open data about the 165 train stations in the Italian region of Lazio to categorize them depending on a series of factors, such as the number of check-ins and reviews, and characteristics of the nearby area such as traffic and population, and the number of significant venues in a radius of 1 km from the station, such as number of restaurants, museums, universities, cafes, professional buildings, hotels, shops, gyms and more.\n",
    "This categorization will be useful to compare the current classification made by RFI with the one deriving from the above parameters, to understand if some station should deserve a different status, such as a promotion or a demotion.\n",
    "The main target audience will be, therefore, RFI management. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data <a name=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below reports the type of data needed and their sources.\n",
    "\n",
    "| Data | Source | Last updated |\n",
    "| --- | --- | --- |\n",
    "| List of RFI train stations in the Lazio region, with location and classification | http://www.rfi.it/rfi/LINEE-STAZIONI-TERRITORIO/Nelle-regioni/Lazio | 2019 |\n",
    "| Number of check-ins and reviews of each station | FourSquare API | Daily updates |\n",
    "| Top 100 venues in a 1000 meters range of each station, categorized by high-level groups | FourSquare API | Daily updates |\n",
    "| Population and density of the neighborhoods hosting the stations, inside Rome | https://it.wikipedia.org/wiki/Municipi_di_Roma | 2017 |\n",
    "| Population and density of the neighborhoods hosting the stations, outside Rome | https://www.tuttitalia.it/lazio/27-comuni/popolazione/ | 2019 |\n",
    "| Pollution levels (as a rough indicator of traffic) | http://www.arpalazio.net/main/aria/sci/qa/misure/PM10.php | 2019 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports and installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing geocoder, geopy and folium\n",
    "!pip3 install geocoder --user\n",
    "!pip3 install geopy --user\n",
    "!pip3 install folium --user\n",
    "\n",
    "# importing pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# importing requests for web page retrieval\n",
    "import requests\n",
    "\n",
    "# importing Beautiful soup for web page parsing\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "# importing matplotlib for map colors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# importing math for NaN processing\n",
    "import math\n",
    "\n",
    "# importing geocoder, Nominatim and folium for map generation\n",
    "import geocoder \n",
    "from geopy.geocoders import Nominatim \n",
    "import folium\n",
    "\n",
    "# importing KMeans for clustering\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing xlwt for excel export\n",
    "!pip3 install xlwt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the stations from the web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading stations from the web page\n",
    "\n",
    "# source url\n",
    "url = \"http://www.rfi.it/rfi/LINEE-STAZIONI-TERRITORIO/Nelle-regioni/Lazio\"\n",
    "\n",
    "# performing the request\n",
    "file = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the text with BeautifulSoup to retrieve the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing data with Beautiful Soup\n",
    "parsable_file = BS(file, 'lxml')\n",
    "\n",
    "# retrieving the table\n",
    "data_table_list = parsable_file.find_all('table')\n",
    "data_table = data_table_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the table into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the table into a list\n",
    "list = pd.read_html(str(data_table), header=0)\n",
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the list into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list into a dataframe\n",
    "df_stations = list[0]\n",
    "df_stations.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping useless rows and columns and giving the dataframe its final shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the row with useless repetitions (STAZIONI(*))\n",
    "df_stations = df_stations.drop([0])\n",
    "df_stations = df_stations.drop([167])\n",
    "\n",
    "# setting the proper row as column headers (Nome stazione/Fermata...)\n",
    "df_stations.columns = df_stations.iloc[0]\n",
    "\n",
    "# dropping the duplicate header\n",
    "df_stations = df_stations.drop([1])\n",
    "\n",
    "df_stations.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the column reporting the company managing the station\n",
    "df_stations = df_stations.drop('Network', axis=1)\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving stations to excel\n",
    "df_stations.to_excel(r'stations.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Rome neighbourhoods from the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading neighbourhoods from the web page\n",
    "\n",
    "# source url\n",
    "url_neigh = \"https://it.wikipedia.org/wiki/Municipi_di_Roma\"\n",
    "\n",
    "# performing the request\n",
    "file_neigh = requests.get(url_neigh).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the text with BeautifulSoup to retrieve the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing data with Beautiful Soup\n",
    "parsable_file = BS(file_neigh, 'lxml')\n",
    "\n",
    "# retrieving the table\n",
    "data_table_list = parsable_file.find_all('table')\n",
    "data_table = data_table_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the table into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the table into a list\n",
    "list = pd.read_html(str(data_table), header=0)\n",
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the list into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list into a dataframe\n",
    "df_rome_neigh = list[0]\n",
    "df_rome_neigh.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting un-necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rome_neigh = df_rome_neigh.drop(['Superficie(km²)', 'Presidente','Corrispondenza con Municipi 2001-2013e Circoscrizioni', 'Suddivisioni amministrative'], axis = 1)\n",
    "df_rome_neigh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving Rome neighbours to excel\n",
    "df_rome_neigh.to_excel(r'rome_neighbourhoods.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Lazio neighbourhoods from the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Lazio neighbourhoods (municipi) from the web page\n",
    "\n",
    "# source url\n",
    "url_munic = \"https://www.tuttitalia.it/lazio/27-comuni/popolazione/\"\n",
    "\n",
    "# performing the request\n",
    "file_munic = requests.get(url_munic).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the text with BeautifulSoup to retrieve the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing data with Beautiful Soup\n",
    "parsable_file = BS(file_munic, 'lxml')\n",
    "\n",
    "# retrieving the table\n",
    "data_table_list = parsable_file.find_all('table')\n",
    "data_table = data_table_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the table into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the table into a list\n",
    "list = pd.read_html(str(data_table), header=0)\n",
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the list into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list into a dataframe\n",
    "df_rome_munic = list[0]\n",
    "df_rome_munic.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting un-necessary columns/rows and fixing province/city mixed up error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping un-necessary columns\n",
    "df_rome_munic = df_rome_munic.drop(['Unnamed: 0', 'Superficiekm²','Altitudinem s.l.m.'], axis = 1)\n",
    "df_rome_munic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping un-necessary rows\n",
    "df_rome_munic = df_rome_munic[:50]\n",
    "df_rome_munic.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_first_two(field):\n",
    "    field = field[0:2]\n",
    "    return field\n",
    "\n",
    "def split_last_two(field):\n",
    "    field = field[2:]\n",
    "    return field\n",
    "\n",
    "\n",
    "df_rome_munic['Provincia'] = df_rome_munic['Comune'].apply(split_first_two)\n",
    "df_rome_munic['Comune'] = df_rome_munic['Comune'].apply(split_last_two)\n",
    "df_rome_munic.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Lazio neighbours to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving Lazio neighbours to excel\n",
    "df_rome_munic.to_excel(r'lazio_neighbourhoods.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pollution from the excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pollutions from the excel\n",
    "\n",
    "# source filepath\n",
    "filepath = \"lazio_poll.xls\"\n",
    "\n",
    "# populating the dataframe\n",
    "df_poll = pd.read_excel(filepath)\n",
    "\n",
    "df_poll.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the un-necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poll = df_poll.drop(['PM10 agosto'], axis = 1)\n",
    "df_poll.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving pollutions to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving pollutions to excel\n",
    "df_poll.to_excel(r'lazio_poll.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology <a name=\"methodology\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each station, from the address will be derived the latitude and longitude, and those in turn will be used to retrieve the FourSquare data and venues. The match with population, density and pollution data will be done on the basis of the neighborhood hosting the station, using a closest distance function on longitude and latitude for joining tables.\n",
    "The above data will be collected in a single dataframe, with a row for each station and columns reporting population, density, pollution and a set of FourSquare categories with the number of top venues occurring for each category.\n",
    "The features will be standardized for better manipulation of the clustering algorithm, and then a clustering will be performed with 4 clusters, the same number of the classes used by RFI.\n",
    "The original classes and the new clusters will then be compared, using both tables and maps, to check for correlations between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading stations from the excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading stations from the excel\n",
    "\n",
    "# source filepath\n",
    "filepath = \"stations.xls\"\n",
    "\n",
    "# populating the dataframe\n",
    "df_stations = pd.read_excel(filepath)\n",
    "\n",
    "df_stations.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving a list containing address (for control), Longitude and Latitude of each train station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving longitude and latitude of each train station\n",
    "\n",
    "# instantiating a geolocator\n",
    "geolocator = Nominatim(user_agent=\"stat_explorer\")\n",
    "count = 0\n",
    "# retrieving data for each station\n",
    "columns_list = []\n",
    "\n",
    "for address, city, name in zip(df_stations['Indirizzo'], df_stations['Comune/Località'], df_stations['Nome Stazione/fermata']):\n",
    "    complete_address = address + ',' + city\n",
    "    # passing the location to the geolocator\n",
    "    location = geolocator.geocode(complete_address)\n",
    "    if location is None:\n",
    "        location = geolocator.geocode(str(name))  \n",
    "        print('nome '+name)\n",
    "    else: print('complete_address '+complete_address)\n",
    "    # retrieving latitude and longitude from the geolocator\n",
    "    \n",
    "    \n",
    "    print(location)\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    latitude = location.latitude\n",
    "    longitude = location.longitude\n",
    "    display_name = location.address\n",
    "    columns_list.append([latitude, longitude, display_name])\n",
    "\n",
    "columns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the retrieved list in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_df = pd.DataFrame(columns_list)\n",
    "columns_df.columns=['latitude', 'longitude', 'address']\n",
    "columns_df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding coordinates to the original table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_coord = pd.concat([df_stations, columns_df], axis=1, sort=False)\n",
    "df_stations_coord.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all un-necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_coord = df_stations_coord.drop(['Unnamed: 0', 'Indirizzo', 'Comune/Località', 'Provincia', 'address'], axis = 1)\n",
    "df_stations_coord.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pollutions from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pollutions from the excel\n",
    "\n",
    "# source filepath\n",
    "filepath = \"lazio_poll.xls\"\n",
    "\n",
    "# populating the dataframe\n",
    "df_poll = pd.read_excel(filepath)\n",
    "\n",
    "df_poll.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving a list containing address (for control), Longitude and Latitude of each pollution measuring station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving longitude and latitude of each pollution station\n",
    "\n",
    "# instantiating a geolocator\n",
    "geolocator = Nominatim(user_agent=\"stat_explorer\")\n",
    "count = 0\n",
    "# retrieving data for each station\n",
    "columns_list = []\n",
    "\n",
    "for address, city in zip(df_poll['Centralina'], df_poll['Provincia']):\n",
    "    complete_address = address + ',' + city\n",
    "    # passing the location to the geolocator\n",
    "    location = geolocator.geocode(complete_address)\n",
    "    if location is None:\n",
    "        location = geolocator.geocode(str(address))  \n",
    "        print('nome '+address)\n",
    "    else: print('complete_address '+complete_address)\n",
    "    # retrieving latitude and longitude from the geolocator\n",
    "    \n",
    "    \n",
    "    print(location)\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    latitude = location.latitude\n",
    "    longitude = location.longitude\n",
    "    display_name = location.address\n",
    "    columns_list.append([latitude, longitude, display_name])\n",
    "\n",
    "columns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the retrieved list in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_df = pd.DataFrame(columns_list)\n",
    "columns_df.columns=['latitude', 'longitude', 'address']\n",
    "columns_df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding coordinates to the original table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poll_coord = pd.concat([df_poll, columns_df], axis=1, sort=False)\n",
    "df_poll_coord.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping un-necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poll_coord = df_poll_coord.drop(['Provincia', 'address'], axis = 1)\n",
    "df_poll_coord.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading neighbourhoods and populations from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading neighbourhoods and populations from the excel\n",
    "\n",
    "# source filepath\n",
    "filepath = \"locations_populations.xls\"\n",
    "\n",
    "# populating the dataframe\n",
    "df_pop = pd.read_excel(filepath)\n",
    "\n",
    "df_pop.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving a list containing address (for control), Longitude and Latitude of each neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving longitude and latitude of each neighbourhood\n",
    "\n",
    "# instantiating a geolocator\n",
    "geolocator = Nominatim(user_agent=\"stat_explorer\")\n",
    "count = 0\n",
    "# retrieving data for each neighbourhood\n",
    "columns_list = []\n",
    "\n",
    "for address, provincia in zip(df_pop['address'], df_pop['Provincia']):\n",
    "    complete_address = address + ',' + city\n",
    "    # passing the location to the geolocator\n",
    "    location = geolocator.geocode(complete_address)\n",
    "    if location is None:\n",
    "        location = geolocator.geocode(str(address))  \n",
    "        print('nome '+address)\n",
    "    else: print('complete_address '+complete_address)\n",
    "    # retrieving latitude and longitude from the geolocator\n",
    "    \n",
    "    \n",
    "    print(location)\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    latitude = location.latitude\n",
    "    longitude = location.longitude\n",
    "    display_name = location.address\n",
    "    columns_list.append([latitude, longitude, display_name])\n",
    "\n",
    "columns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the retrieved list in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_df = pd.DataFrame(columns_list)\n",
    "columns_df.columns=['latitude', 'longitude', 'address']\n",
    "columns_df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding coordinates to the original table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop_coord = pd.concat([df_pop, columns_df], axis=1, sort=False)\n",
    "df_pop_coord.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping un-necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop_coord = df_pop_coord.drop(['Unnamed: 0','Provincia', 'address'], axis = 1)\n",
    "df_pop_coord.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging together: stations, pollution and population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning at each station the correct pollution and population values using latitude and longitude\n",
    "\n",
    "# importing the geopy function calculating distance of two points given the coordinates\n",
    "import geopy.distance\n",
    "\n",
    "# maximum distance between points on Earth\n",
    "max_distance = 20000\n",
    "\n",
    "# this function returns PM10 average and number of days above max level for given coordinates\n",
    "def find_pollution(latitude, longitude):\n",
    "\n",
    "    # station coordinates\n",
    "    stat_coord = (latitude, longitude)\n",
    "    \n",
    "    temp_distance = max_distance\n",
    "    \n",
    "    pollution = []\n",
    "    \n",
    "    for lat, long, pm10, bre in zip(df_poll_coord['latitude'], df_poll_coord['longitude'], df_poll_coord['PM10 media'], df_poll_coord['Sforamenti']):\n",
    "        # pollution station coordinates\n",
    "        poll_coord = (lat, long)\n",
    "        # distance between points according to the Vincenty's formula\n",
    "        distance = geopy.distance.vincenty(stat_coord, poll_coord).km\n",
    "        if distance < temp_distance: \n",
    "            temp_distance = distance\n",
    "            temp_PM10 = pm10\n",
    "            temp_sforamenti = bre\n",
    "    \n",
    "    temp_PM10  \n",
    "    temp_sforamenti\n",
    "    \n",
    "    return temp_PM10, temp_sforamenti\n",
    "\n",
    "# this function returns population total and density for given coordinates\n",
    "def find_population(latitude, longitude):\n",
    "\n",
    "    # station coordinates\n",
    "    stat_coord = (latitude, longitude)\n",
    "    \n",
    "    temp_distance = max_distance\n",
    "    \n",
    "    population = []\n",
    "    for lat, long, pop, dens in zip(df_pop_coord['latitude'], df_pop_coord['longitude'], df_pop_coord['Popolazione(ab)'], df_pop_coord['Densità(ab/km²)']):\n",
    "        # neighbourhood coordinates\n",
    "        pop_coord = (lat, long)\n",
    "        # distance between points according to the Vincenty's formula\n",
    "        distance = geopy.distance.vincenty(stat_coord, pop_coord).km\n",
    "        if distance < temp_distance: \n",
    "            temp_distance = distance\n",
    "            temp_population = pop\n",
    "            temp_density = dens\n",
    "    \n",
    "    temp_population  \n",
    "    temp_density\n",
    "    \n",
    "    return temp_population, temp_density\n",
    "\n",
    "columns_list = []\n",
    "count = 0\n",
    "\n",
    "# finding the pollution and population values for each station\n",
    "for latitude, longitude in zip(df_stations_coord['latitude'], df_stations_coord['longitude']):\n",
    "    \n",
    "    # finding the pollution values corresponding to the station\n",
    "    pm10, overcoming = find_pollution(latitude, longitude)\n",
    "    \n",
    "    # finding the population values corresponding to the station\n",
    "    population, density = find_population(latitude, longitude)\n",
    "    \n",
    "    count = count + 1\n",
    "    print(pm10)\n",
    "    print(overcoming)\n",
    "    print(population)\n",
    "    print(density)\n",
    "    print(latitude) \n",
    "    print(longitude) \n",
    "    columns_list.append([pm10, overcoming, population, density])\n",
    "\n",
    "columns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_df = pd.DataFrame(columns_list)\n",
    "columns_df.columns=['PM10', 'overcomings', 'population', 'density']\n",
    "columns_df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding data to the original table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_coord_pop_poll = pd.concat([df_stations_coord, columns_df], axis=1, sort=False)\n",
    "df_stations_coord_pop_poll.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the venues for each station, by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing my FourSquare credentials in variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my foursquare credentials\n",
    "CLIENT_ID = 'GJAYQ42Z4F1LCF2G2QRREQUEF5B0YD0HC1CIGM3ZXEZWYVWD' # my Foursquare ID\n",
    "CLIENT_SECRET = '3NG0CMKA3LYWVW3ZUXXLDMF33DNJUKHNUJPMVGPMKTTFSIM3' # my Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "print('My credentials:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the 10 high-level categories of venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the url requesting all the categories\n",
    "categories_url = 'https://api.foursquare.com/v2/venues/categories?client_id={}&client_secret={}&v={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION)\n",
    "\n",
    "# executing the request\n",
    "results = requests.get(categories_url).json()\n",
    "\n",
    "number_high_categories = len(results['response']['categories'])\n",
    "print('number of high-level categories: '+ str(number_high_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizializing the list\n",
    "categories_list = []  \n",
    "\n",
    "# storing the high-level categories into a list\n",
    "for i in range(0,number_high_categories):\n",
    "    id = results['response']['categories'][i]['id']\n",
    "    name = results['response']['categories'][i]['name']\n",
    "    print(id)\n",
    "    print(name)\n",
    "    categories_list.append([name, id])\n",
    "\n",
    "#showing the list content\n",
    "print(categories_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding columns to the stations dataframe to store the venues data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_venues = df_stations_coord_pop_poll.copy()\n",
    "\n",
    "# for each categoy, a new empty column is created\n",
    "for categories in categories_list:\n",
    "    # the empty column creation\n",
    "    df_stations_venues[categories[0]] = 0\n",
    "df_stations_venues.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the venues count for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the calls to stay in the free account\n",
    "\n",
    "# a function returning the number of venues belonging to a category and close to the given lat and long\n",
    "def get_total_venues_count(latitude, longitude, categoryId, radius):\n",
    "    # latitude and longitude are joined in a single coordinates field\n",
    "    latlong = str(latitude) + ',' + str(longitude)\n",
    "    # the url is built\n",
    "    explore_url = 'https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&v={}&ll={}&radius={}&categoryId={}&limit={}'.format(\n",
    "                CLIENT_ID, \n",
    "                CLIENT_SECRET, \n",
    "                VERSION,\n",
    "                latlong,\n",
    "                radius,\n",
    "                categoryId)\n",
    "        \n",
    "    # the request is executed, asking for the totalResults, that is the needed venues count\n",
    "    return requests.get(explore_url).json()['response']['totalResults']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the venues count\n",
    "count = 1\n",
    "for i, row in df_stations_venues.iterrows():\n",
    "    for category in categories_list:\n",
    "        df_stations_venues.loc[i, category[0]] = get_total_venues_count(df_stations_venues.latitude.iloc[i],df_stations_venues.longitude.iloc[i], categoryId = category[1], radius = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the venues to excel\n",
    "df_stations_venues.to_excel(r'stations_venues.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis <a name=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stations with full data are loaded and displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading stations from the excel\n",
    "\n",
    "# source filepath\n",
    "filepath = \"stations_venues.xls\"\n",
    "\n",
    "# populating the dataframe\n",
    "df_stations = pd.read_excel(filepath)\n",
    "df_stations = df_stations.drop('Unnamed: 0', axis=1)\n",
    "df_stations.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure is checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data statistic is explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Events are non significant, the columns is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations = df_stations.drop(['Event'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation='vertical')\n",
    "sns.boxplot\n",
    "\n",
    "df_stations_copy = df_stations.drop(['population', 'density','latitude','longitude'], axis=1)\n",
    "ax = sns.boxplot(data = df_stations_copy)\n",
    "ax.set_ylabel('Count of occurrences', fontsize=25)\n",
    "ax.set_xlabel('Attributes', fontsize=25)\n",
    "ax.tick_params(labelsize=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most represented venues are Food, Professional and Shops, and all venues present significant upper outliers. PM10 and overcomings are more evenly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now standardized as a preparation for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#getting the values to be standardized (excluding name and category)\n",
    "X = df_stations.values[:,4:]\n",
    "# applying scaler\n",
    "scaled_dataset = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the no-more used Event element from the categories list\n",
    "categories_list.remove(['Event', '4d4b7105d754a06373d81259'])\n",
    "categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the scaled dataset in dataframe\n",
    "scaled_dataframe = pd.DataFrame(scaled_dataset)\n",
    "# setting the column names again\n",
    "scaled_dataframe.columns = ['PM10','overcomings', 'population', 'density']+[category[0] for category in categories_list]\n",
    "scaled_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting as before, but with scaled data using pop and density also\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation='vertical')\n",
    "sns.boxplot\n",
    "\n",
    "ax = sns.boxplot(data = scaled_dataframe)\n",
    "ax.set_ylabel('Count of occurrences', fontsize=25)\n",
    "ax.set_xlabel('Attributes', fontsize=25)\n",
    "ax.tick_params(labelsize=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering on 4 clusters, as many as the original categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters\n",
    "clusters = 4\n",
    "\n",
    "# performing the k-means clustering\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=0).fit(scaled_dataframe)\n",
    "\n",
    "# retrieving the labels\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "# Change label numbers so they go from highest scores to lowest\n",
    "replace_labels = {0:2, 1:0, 2:3, 3:1}\n",
    "for i in range(len(kmeans_labels)):\n",
    "    kmeans_labels[i] = replace_labels[kmeans_labels[i]]\n",
    "\n",
    "df_stations_clusters_labels = df_stations.copy()\n",
    "df_stations_clusters_labels['Cluster'] = kmeans_labels\n",
    "stations_clusters_minmax_df = scaled_dataframe.copy()\n",
    "stations_clusters_minmax_df['Cluster'] = kmeans_labels\n",
    "stations_clusters_minmax_df['Nome Stazione/fermata'] = df_stations['Nome Stazione/fermata']\n",
    "stations_clusters_minmax_df['latitude'] = df_stations['latitude']\n",
    "stations_clusters_minmax_df['longitude'] = df_stations['longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the characteristics of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# creating a figure\n",
    "fig, axes = plt.subplots(1,clusters, figsize=(20, 10), sharey=True)\n",
    "\n",
    "# setting y label\n",
    "axes[0].set_ylabel('Count of attributes (relative)', fontsize=25)\n",
    "\n",
    "# building the boxes for each cluster\n",
    "for k in range(clusters):\n",
    "    #Set same y axis limits\n",
    "    axes[k].set_ylim(0,1.1)\n",
    "    axes[k].xaxis.set_label_position('top')\n",
    "    axes[k].set_xlabel('Cluster ' + str(k), fontsize=25)\n",
    "    axes[k].tick_params(labelsize=20)\n",
    "    plt.sca(axes[k])\n",
    "    plt.xticks(rotation='vertical')\n",
    "    # filling the box with boxplots\n",
    "    sns.boxplot(data = stations_clusters_minmax_df[stations_clusters_minmax_df['Cluster'] == k].drop('Cluster',1), ax=axes[k])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cluster shows an excellent presence in all categories, while the second is significantly below. \n",
    "The third and fourth ones have smaller indicators and are differentiated mostly by distribution of venues\n",
    "and population/pollution differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results to excel\n",
    "df_stations_clusters_labels.to_excel(r'stations_labeled.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading labeled stations from the excel\n",
    "\n",
    "# source filepath\n",
    "filepath = \"stations_labeled.xls\"\n",
    "\n",
    "# populating the dataframe\n",
    "df_stations = pd.read_excel(filepath)\n",
    "df_stations = df_stations.drop('Unnamed: 0', axis=1)\n",
    "df_stations.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping categories to numbers for easier confrontation\n",
    "mapping = {'PLATINUM': 0, 'GOLD': 1, 'SILVER': 2, 'BRONZE': 3}\n",
    "df_stations = df_stations.replace({'Categoria': mapping})\n",
    "df_stations.to_excel(r'stations_labeled.xls')\n",
    "df_stations.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the correctly labeled stations\n",
    "df_correctly_labeled = df_stations[df_stations['Categoria'] == df_stations['Cluster']]\n",
    "print(str(len(df_correctly_labeled)) + ' stations out of 165 were correctly labeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the uncorrectly labeled stations\n",
    "df_uncorrectly_labeled = df_stations[df_stations['Categoria'] != df_stations['Cluster']]\n",
    "df_uncorrectly_labeled = df_uncorrectly_labeled[['Nome Stazione/fermata', 'Categoria', 'Cluster']]\n",
    "print(str(len(df_uncorrectly_labeled)) + ' stations out of 165 were uncorrectly labeled')\n",
    "df_uncorrectly_labeled.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84% of stations matched the original RFI classification.\n",
    "16% of stations fell into the category immediately below or above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the map with original RFI classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rome coordinates\n",
    "\n",
    "latitude = 41.9028\n",
    "longitude = 12.4964\n",
    "\n",
    "# creating map\n",
    "map_rfi = folium.Map(location=[latitude, longitude], zoom_start=9)\n",
    "\n",
    "# setting color scheme for the clusters\n",
    "x = np.arange(clusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(clusters)] \n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys))) \n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# adding markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, categoria in zip(df_stations['latitude' ], df_stations['longitude'], df_stations['Nome Stazione/fermata'], df_stations['Categoria']):\n",
    "    label = folium.Popup(str(poi) + ' Categoria ' + str(categoria), parse_html=True)\n",
    "    if math.isnan(categoria): categoria=5\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label, \n",
    "        color=rainbow[int(categoria-1)],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[int(categoria-1)],\n",
    "        fill_opacity=0.7).add_to(map_rfi)\n",
    "map_rfi\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the map with original cluster classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rome coordinates\n",
    "\n",
    "latitude = 41.9028\n",
    "longitude = 12.4964\n",
    "\n",
    "# creating map\n",
    "map_rfi = folium.Map(location=[latitude, longitude], zoom_start=9)\n",
    "\n",
    "# setting color scheme for the clusters\n",
    "x = np.arange(clusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(clusters)] \n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys))) \n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# adding markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, categoria in zip(df_stations['latitude' ], df_stations['longitude'], df_stations['Nome Stazione/fermata'], df_stations['Cluster']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(categoria), parse_html=True)\n",
    "    if math.isnan(categoria): categoria=5\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label, \n",
    "        color=rainbow[int(categoria-1)],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[int(categoria-1)],\n",
    "        fill_opacity=0.7).add_to(map_rfi)\n",
    "map_rfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion <a name=\"results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of the project was to use data gathered from Foursquare and other sources to classify the stations independently and understand if some stations should deserve a different status from the one given by the network manager (RFI = Rete Ferroviaria Italiana), according to the characteristics of the nearby area.\n",
    "\n",
    "RFI, in fact, arranged the stations into four categories: Platinum, Gold, Silver and Bronze, depending on the level of services offered in each stations.\n",
    "\n",
    "The list of the 165 train stations of the Italian region named Lazio was successfully websourced, as well as their addresses. \n",
    "\n",
    "Two more datasets were websourced:\n",
    "\n",
    "1. a list of pollution measuring stations, with average yearly emissions (PM10) and number of days when the maximum threshold has been trespassed (overcomings); these data were used mainly as indicators of traffic;\n",
    "2. a list of all main neighbourhoods in Lazio, with the overall population and density.\n",
    "\n",
    "Using Nominatim, the address of the three POIs above were converted into geographical coordinates (longitude and latitude).\n",
    "\n",
    "After that, a function based on the Vincenty geodesic distance was employed, to associate the closest pollution station and the closest neighbourhood, as well as their data, to each train station.\n",
    "\n",
    "The final step of the data collection used FourSquare to gather, for each station, the number of venues in a range of 1500 meters, arranged in the FourSquare top 10 categories, that are:\n",
    "\n",
    "- Arts & Entertainment\n",
    "- College & University\n",
    "- Event\n",
    "- Food\n",
    "- Nightlife Spot\n",
    "- Outdoors & Recreation\n",
    "- Professional & Other Places\n",
    "- Residence\n",
    "- Shop & Service\n",
    "- Travel & Transport\n",
    "\n",
    "After scaling the data, the K-Means clustering was applied, using a 4 clusters, the same number of the RFI categories.\n",
    "\n",
    "The visual analysis of the clusters showed a first cluster with an excellent presence in all categories, with the second significantly below. The third and fourth ones had smaller quantities in all venue indicators and were differentiated mostly by distribution of venues and population/pollution differences.\n",
    "\n",
    "84% of stations matched the original RFI classification. 16% of stations fell into the category immediately below or above. This overall agreement between original categories and computed clusters was shown by visual maps as well.\n",
    "\n",
    "The stations that were differently classified fell mostly into 2 categories:\n",
    "\n",
    "1. stations in the centre of Rome, with a venue rich neighbourhood, promoted to the above category (such as Trastevere, S. Pietro, Aurelia, Quattro Venti, Balduina, Monte Mario)\n",
    "2. stations playing a crucial role as a node in the transportation network, but with poorer neighbourhood, that were demoted to the category immediately below (such as Tiburtina, Fiumicino, Ciampino, Civitavecchia)\n",
    "\n",
    "Therefore, nothwithstanding the fact that FourSquare Data is un-balanced, with some categories, such as food, over-represented, the clustering made quite sense, with the stations distribution matching broadly the investments made by the network manager on each station, as reflected by the category level.\n",
    "\n",
    "The non matching labels made sense as well, showing that some stations deserved a higher level than the one suggested by the nearby area, given the strategic position (a close-by airport or harbour), while others should deserve a promotion considering the amount of venues and activities in the nearby area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project made use of websourced data and a non-commercial FourSquare account.\n",
    "\n",
    "Notwithstanding these limited resources, it proved strong insights into the areas surrounding each station, supporting the choices made by the rail network manager and giving some valuable suggestion.\n",
    "\n",
    "Further applications could be:\n",
    "\n",
    "1. exploring the predicted impact of a station promotion/demotion on its reviews;\n",
    "2. exploring the correlation of check-ins and reviews to other parameters, like availability of certain venues, such as parking or bus stops or restaurants, or the lack of others, such as coworking places or professional structures;\n",
    "3. the creation of a recommendation system able to advice citizens and tourists about the nearby venues of each station, depending on their needs and past choices of similar users.\n",
    "\n",
    "The project could be extended to the whole Italian network as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
